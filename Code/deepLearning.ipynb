{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deepLearning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPo70pq/rSC5y4LRSB94P2m"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qDSD7vM6wZVP","executionInfo":{"status":"ok","timestamp":1655295332326,"user_tz":-300,"elapsed":2953,"user":{"displayName":"Burhan Abbasi","userId":"17748489866286671220"}},"outputId":"0fc5df76-d95d-4b2a-f2a0-c268937825f3"},"source":["#Access to Drive\n","from google.colab import drive\n","\n","drive.mount ('/content/gdrive')\n","\n","# to attempt to forcibly remount\n","# drive.mount(\"/content/gdrive\", force_remount=True)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"xjDM0pXNg6aF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655294145506,"user_tz":-300,"elapsed":6034,"user":{"displayName":"Burhan Abbasi","userId":"17748489866286671220"}},"outputId":"bafecbbb-9474-4af0-e9dc-e80a7ede0340"},"source":["#Library installations\n","!pip install torchtext"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n","Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.11.0+cu113)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchtext) (4.2.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.5.18.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n"]}]},{"cell_type":"code","source":["#imports\n","import sys\n","import os\n","\n","py_file_location = \"/content/gdrive/MyDrive/SEO Schema.org/Code/\"\n","sys.path.append(os.path.abspath(py_file_location))\n","\n","import numpy as np\n","import pandas as pd\n","import spacy\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torchtext.legacy.data import Field, LabelField\n","from torchtext.legacy.data import TabularDataset\n","from torchtext.legacy.data import BucketIterator\n","\n","from sklearn.metrics import confusion_matrix\n","\n","\n","\n","import clsLSTM\n","import ExportFeatureSet\n"],"metadata":{"id":"kCH_34YrAE6O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spacy_en = spacy.load(\"en\")\n","\n","def tokenize(text):\n","    return [tok.text for tok in spacy_en.tokenizer(text)]\n","\n","\n","\n","TEXT = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\n","LABEL = LabelField(dtype = torch.long, use_vocab=False) \n","\n","fields = {\"text\": (\"txt\", TEXT), \"label\": (\"lbl\", LABEL)}\n","train_data, test_data = TabularDataset.splits(path=\"../DataSet/Dictionary/\", \n","                                              train=\"Strong_Train.csv\", \n","                                              test=\"Strong_Test.csv\", format=\"csv\", fields=fields)\n","TEXT.build_vocab(train_data, max_size=10000, min_freq=1)\n","LABEL.build_vocab(train_data)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#For LSTM\n","train_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, test_data), batch_size=1, device=device, sort=False \n",")"],"metadata":{"id":"c-m_jaQY_4zV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","input_size = len(TEXT.vocab)\n","hidden_size = 512\n","num_layers = 2\n","embedding_size = 100\n","learning_rate = 0.005\n","num_epochs =1\n","num_classes = 7 \n","# Initialize network\n","model = clsLSTM(input_size, embedding_size, hidden_size, num_layers,num_classes).to(device)\n","#print(model)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"6gHUBj_SBAE-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_loss= {'train':[],'test':[]}\n","y_pred = [] #for cofusion matrix\n","y_true = [] #for cofusion matrix\n","\n","# Train Network\n","for epoch in range(num_epochs):\n","  epoch_losses = list()\n","  for batch_idx, batch in enumerate(train_iterator):\n","    # Get data to cuda if possible\n","    data = batch.txt.to(device=device)\n","    \n","    targets = batch.lbl.to(device=device)\n","\n","    # forward\n","    scores = model(data)\n","\n","    loss = criterion(scores, targets)\n","    # loss = criterion(scores.squeeze(1), targets.type_as(scores))\n","\n","    # backward\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    # gradient descent\n","    optimizer.step()\n","\n","    epoch_losses.append(loss.item())\n","\n","    \n","  _loss['train'].append(np.mean(epoch_losses))\n","  print('train loss on epoch {} : {:.3f}'.format(epoch, np.mean(epoch_losses)))\n","\n","\n","  test_losses = list()\n","  _totalPredictions=0\n","  _correctPredictions=0\n","  for batch in test_iterator:\n","    with torch.no_grad():\n","      optimizer.zero_grad()\n","      prediction = model(batch.txt)    \n","      _totalPredictions=_totalPredictions+1\n","      \n","      if(torch.argmax(prediction)== batch.lbl):\n","        _correctPredictions= _correctPredictions+1\n","      loss = criterion(prediction, batch.lbl)\n","      test_losses.append(loss.item())\n","\n","      y_pred.append(int(prediction.argmax(dim=1)[0]))\n","      y_true.append(int(batch.lbl))\n","\n","\n","  _loss['test'].append(np.mean(test_losses))    \n","  print('test loss on epoch {}: {:.3f}'.format(epoch, np.mean(test_losses)))\n","  print('{} / {} = {:.3f}'.format(_correctPredictions, _totalPredictions, _correctPredictions/ _totalPredictions))\n","  "],"metadata":{"id":"oku9L4MMBYoO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Confusion Matrix\n","\n","# label,entity\n","# 0,address\n","# 1,personName\n","# 2,email\n","# 3,contact\n","# 4,title\n","# 5,domain\n","# 6,qualification\n","\n","confusion_matrix(y_true, y_pred, labels=[0,1,2,3,4,5,6])\n"],"metadata":{"id":"Lvn9zk2aCUnX"},"execution_count":null,"outputs":[]}]}